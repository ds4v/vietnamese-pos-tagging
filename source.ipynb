{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import unicodedata as ud\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dữ liệu thu thập"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = open('dataset/sentences.txt', encoding='utf-8').readlines()\n",
    "tokenize_sentences = [sentence.split(' ') for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng câu đã thu thập: 60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Pha lập công trên đã giúp Rashford giải hạn bàn thắng tại sân Old Trafford kéo dài 845 phút .\\n',\n",
       " 'Với 3 điểm có được trong trận đấu này , Quỷ đỏ đã leo lên vị trí thứ 2 trên bảng xếp hạng Premier League với 30 điểm , chỉ kém đội đầu bảng Liverpool 2 điểm .\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Số lượng câu đã thu thập:', len(sentences))\n",
    "sentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu có số từ nhiều nhât: 46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Những thực phẩm được chế biến như hun khói , thức ăn ngâm tẩm , muối , món ăn chứa lượng muối cao thường có tỷ lệ mắc ung thư dạ dày cao hơn những người có thói quen ăn uống nhạt và thanh đạm .\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_str = max(tokenize_sentences, key=len)\n",
    "print('Câu có số từ nhiều nhât:', len(max_str))\n",
    "' '.join(max_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu có số từ ít nhât: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nhiều người có hoàn cảnh giống ông .\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_str = min(tokenize_sentences, key=len)\n",
    "print('Câu có số từ ít nhât:', len(min_str))\n",
    "' '.join(min_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tách từ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sử dụng thuật toán Longest Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllablize(sentence):\n",
    "    word = '\\w+'\n",
    "    non_word = '[^\\w\\s]'\n",
    "    digits = '\\d+([\\.,_]\\d+)+'\n",
    "    \n",
    "    patterns = []\n",
    "    patterns.extend([word, non_word, digits])\n",
    "    patterns = f\"({'|'.join(patterns)})\"\n",
    "    \n",
    "    sentence = ud.normalize('NFC', sentence)\n",
    "    tokens = re.findall(patterns, sentence, re.UNICODE)\n",
    "    return [token[0] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_n_grams(path):\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        words = f.read()\n",
    "        words = ast.literal_eval(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_matching(sentence, bi_grams, tri_grams):\n",
    "    syllables = syllablize(sentence)\n",
    "    syl_len = len(syllables)\n",
    "    \n",
    "    curr_id = 0\n",
    "    word_list = []\n",
    "    done = False\n",
    "    \n",
    "    while (curr_id < syl_len) and (not done):\n",
    "        curr_word = syllables[curr_id]\n",
    "        if curr_id >= syl_len - 1:\n",
    "            word_list.append(curr_word)\n",
    "            done = True\n",
    "        else:\n",
    "            next_word = syllables[curr_id + 1]\n",
    "            pair_word = ' '.join([curr_word.lower(), next_word.lower()])\n",
    "            if curr_id >= (syl_len - 2):\n",
    "                if pair_word in bi_grams:\n",
    "                    word_list.append('_'.join([curr_word, next_word]))\n",
    "                    curr_id += 2\n",
    "                else:\n",
    "                    word_list.append(curr_word)\n",
    "                    curr_id += 1\n",
    "            else:\n",
    "                next_next_word = syllables[curr_id + 2]\n",
    "                triple_word = ' '.join([pair_word, next_next_word.lower()])\n",
    "                if triple_word in tri_grams:\n",
    "                    word_list.append('_'.join([curr_word, next_word, next_next_word]))\n",
    "                    curr_id += 3\n",
    "                elif pair_word in bi_grams:\n",
    "                    word_list.append('_'.join([curr_word, next_word]))\n",
    "                    curr_id += 2\n",
    "                else:\n",
    "                    word_list.append(curr_word)\n",
    "                    curr_id += 1\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nhưng', 'sự_thực', 'hiện', 'vẫn', 'còn', 'chưa', 'phù_hợp']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_grams = load_n_grams('resources/bi_grams.txt')\n",
    "tri_grams = load_n_grams('resources/tri_grams.txt')\n",
    "longest_matching('nhưng sự thực hiện vẫn còn chưa phù hợp', bi_grams, tri_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pha lập_công trên đã giúp Rashford giải hạn bàn thắng tại sân Old Trafford kéo dài 845 phút .',\n",
       " 'Với 3 điểm có được trong trận đấu này , Quỷ đỏ đã leo lên vị_trí thứ 2 trên bảng xếp_hạng Premier League với 30 điểm , chỉ kém đội đầu_bảng Liverpool 2 điểm .',\n",
       " 'Tổng_thống đắc_cử Joe Biden được cho đang cân_nhắc việc cắt bỏ chương_trình hiện_đại hóa hạt_nhân trị_giá 1 . 000 tỷ USD do chính_quyền Tổng_thống đương_nhiệm Donald Trump đề_xuất .']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tokenize/longest_matching_tokens.txt', 'w', encoding='utf-8') as f:\n",
    "    longest_matching_sentences = []\n",
    "    for sentence in sentences:\n",
    "        word_list = longest_matching(sentence, bi_grams, tri_grams)\n",
    "        longest_matching_sentences.append(' '.join(word_list))\n",
    "        for word in word_list: f.write(word + '\\n')\n",
    "        if sentence != sentences[-1]: f.write('\\n')\n",
    "    f.write('\\n')\n",
    "longest_matching_sentences[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng từ ghép khi tách từ bằng thuật toán Longest Matching: 257\n"
     ]
    }
   ],
   "source": [
    "count_longest_matching_compounds = 0\n",
    "for sentence in longest_matching_sentences:\n",
    "    for word in sentence.split():\n",
    "        if '_' in word: count_longest_matching_compounds += 1\n",
    "print('Số lượng từ ghép khi tách từ bằng thuật toán Longest Matching:', count_longest_matching_compounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sử dụng thư viện VnCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nhưng', 'sự', 'thực_hiện', 'vẫn', 'còn', 'chưa', 'phù_hợp']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "client = VnCoreNLP(address='http://127.0.0.1', port=9001)\n",
    "word_list = client.tokenize('nhưng sự thực hiện vẫn còn chưa phù hợp')[0]\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pha lập_công trên đã giúp Rashford giải hạn bàn thắng tại sân Old_Trafford kéo_dài 845 phút .',\n",
       " 'Với 3 điểm có được trong trận đấu này , Quỷ đỏ đã leo lên vị_trí thứ 2 trên bảng xếp_hạng Premier_League với 30 điểm , chỉ kém đội đầu_bảng Liverpool 2 điểm .',\n",
       " 'Tổng_thống đắc_cử Joe_Biden được cho đang cân_nhắc việc cắt bỏ chương_trình hiện_đại_hoá hạt_nhân trị_giá 1.000 tỷ USD do chính_quyền Tổng_thống đương_nhiệm Donald_Trump đề_xuất .']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tokenize/vncore_tokens.txt', 'w', encoding='utf-8') as f:\n",
    "    vncore_sentences = []\n",
    "    for sentence in sentences:\n",
    "        word_list = client.tokenize(sentence)[0]\n",
    "        vncore_sentences.append(' '.join(word_list))\n",
    "        for word in word_list: f.write(word + '\\n')\n",
    "        if sentence != sentences[-1]: f.write('\\n')\n",
    "    f.write('\\n')\n",
    "vncore_sentences[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng từ ghép khi tách từ bằng thư viện VnCoreNLP: 282\n"
     ]
    }
   ],
   "source": [
    "count_vncore_compounds = 0\n",
    "for sentence in vncore_sentences:\n",
    "    for word in sentence.split():\n",
    "        if '_' in word: count_vncore_compounds += 1\n",
    "print('Số lượng từ ghép khi tách từ bằng thư viện VnCoreNLP:', count_vncore_compounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thủ công"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pha lập_công trên đã giúp Rashford giải_hạn bàn_thắng tại sân Old_Trafford kéo_dài 845 phút .',\n",
       " 'Với 3 điểm có được trong trận_đấu này , Quỷ_đỏ đã leo_lên vị_trí thứ 2 trên bảng xếp_hạng Premier_League với 30 điểm , chỉ kém đội đầu_bảng Liverpool 2 điểm .',\n",
       " 'Tổng_thống đắc_cử Joe_Biden được cho đang cân_nhắc việc_cắt_bỏ chương_trình hiện_đại_hoá hạt_nhân trị_giá 1.000 tỷ USD do chính_quyền Tổng_thống đương_nhiệm Donald_Trump đề_xuất .']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tokenize/manual_tokens.txt', 'r', encoding='utf-8') as f:\n",
    "    manual_tokenize_sentences = []\n",
    "    sentence = ''\n",
    "    for word in f:\n",
    "        if word == '\\n': \n",
    "            manual_tokenize_sentences.append(sentence.strip())\n",
    "            sentence = ''\n",
    "        else: sentence += word.replace('\\n', ' ')\n",
    "manual_tokenize_sentences[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng từ ghép khi tách từ thủ công: 309\n"
     ]
    }
   ],
   "source": [
    "count_manual_tokenize_compounds = 0\n",
    "for sentence in manual_tokenize_sentences:\n",
    "    for word in sentence.split():\n",
    "        if '_' in word: count_manual_tokenize_compounds += 1\n",
    "print('Số lượng từ ghép khi tách từ thủ công:', count_manual_tokenize_compounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đánh giá kết quả tách từ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_correct_words(pred, source, n_grams=3):\n",
    "    pred_words = pred.split()\n",
    "    source_words = source.split()\n",
    "    \n",
    "    total_true, tp = 0, 0\n",
    "    total_errors, fp = 0, 0\n",
    "    \n",
    "    idx = 0\n",
    "    while idx < len(pred_words):\n",
    "        if pred_words[idx] not in source_words[idx:(idx + n_grams)]: \n",
    "            if '_' in pred_words[idx]: fp += 1\n",
    "            del pred_words[idx]\n",
    "            total_errors += 1\n",
    "        else: idx += 1\n",
    "    \n",
    "    idx = 0\n",
    "    while idx < len(source_words):\n",
    "        if source_words[idx] not in pred_words[idx:(idx + n_grams)]: \n",
    "            del source_words[idx]\n",
    "        else: idx += 1\n",
    "    \n",
    "    if len(pred_words) < len(source_words): words = pred_words\n",
    "    else: words = source_words\n",
    "    \n",
    "    for idx in range (len(words)):\n",
    "        if pred_words[idx] == source_words[idx]:\n",
    "            if '_' in pred_words[idx]: tp += 1 \n",
    "            total_true += 1\n",
    "                    \n",
    "    return total_true, total_errors, tp, fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_evaluation(pred, source, n_grams=3):\n",
    "    total_true = 0\n",
    "    total_errors = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    pred_tp = 0\n",
    "    pred_fp = 0\n",
    "    \n",
    "    for pred_sentence, source_sentence in zip(pred, source):\n",
    "        total_words += len(source_sentence.split())\n",
    "        if pred_sentence != source_sentence:\n",
    "            true, error, tp, fp = count_correct_words(pred_sentence, source_sentence, n_grams)\n",
    "            total_true += true \n",
    "            total_errors += error\n",
    "            pred_tp += tp\n",
    "            pred_fp += fp\n",
    "        else:\n",
    "            for word in source_sentence.split():\n",
    "                if '_' in word: pred_tp += 1\n",
    "                total_true += 1\n",
    "    return {\n",
    "        'Accuracy': total_true / total_words, \n",
    "        'Precision': pred_tp / (pred_tp + pred_fp),\n",
    "        'Recall': pred_tp / count_manual_tokenize_compounds,\n",
    "        'True Positive': pred_tp, \n",
    "        'False Positive': pred_fp,\n",
    "        'Total True': total_true, \n",
    "        'Total Errors': total_errors\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Longest Matching</th>\n",
       "      <th>VnCoreNLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.856044</td>\n",
       "      <td>0.931868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.891051</td>\n",
       "      <td>0.950355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.7411</td>\n",
       "      <td>0.867314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Positive</th>\n",
       "      <td>229</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positive</th>\n",
       "      <td>28</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total True</th>\n",
       "      <td>779</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Errors</th>\n",
       "      <td>210</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Longest Matching VnCoreNLP\n",
       "Accuracy               0.856044  0.931868\n",
       "Precision              0.891051  0.950355\n",
       "Recall                   0.7411  0.867314\n",
       "True Positive               229       268\n",
       "False Positive               28        14\n",
       "Total True                  779       848\n",
       "Total Errors                210        99"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_matching_evaluation = tokenize_evaluation(longest_matching_sentences, manual_tokenize_sentences)\n",
    "vncore_evaluation = tokenize_evaluation(vncore_sentences, manual_tokenize_sentences)\n",
    "pd.DataFrame(\n",
    "    [longest_matching_evaluation, vncore_evaluation], \n",
    "    index = ['Longest Matching', 'VnCoreNLP']\n",
    ").astype(object).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tạo ngữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng từ: 1050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Pha\\n', 'lập_công\\n', 'trên\\n', 'đã\\n', 'giúp\\n']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_matching_tokens = open('tokenize/longest_matching_tokens.txt', encoding='utf-8').readlines()\n",
    "print('Số lượng từ:', len(longest_matching_tokens))\n",
    "longest_matching_tokens[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng từ: 1007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Pha\\n', 'lập_công\\n', 'trên\\n', 'đã\\n', 'giúp\\n']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vncore_tokens = open('tokenize/vncore_tokens.txt', encoding='utf-8').readlines()\n",
    "print('Số lượng từ:', len(vncore_tokens))\n",
    "vncore_tokens[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng từ: 970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Pha\\n', 'lập_công\\n', 'trên\\n', 'đã\\n', 'giúp\\n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_tokens = open('tokenize/manual_tokens.txt', encoding='utf-8').readlines()\n",
    "print('Số lượng từ:', len(manual_tokens))\n",
    "manual_tokens[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tạo dataset bán thủ công: Gán nhãn trước bằng VnCoreNLP sau đó kiểm tra lại nhãn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/gold.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in manual_tokens:\n",
    "        word = word.replace('\\n', '')\n",
    "        \n",
    "        if '_' not in word: tag = client.pos_tag(word)\n",
    "        else: tag = client.pos_tag(word.replace('_', ' '))\n",
    "        \n",
    "        if tag == []: f.write('\\n')\n",
    "        else: f.write(f'{word}\\t{tag[0][0][1]}\\n')\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chia ngữ liệu vừa tạo thành các bộ Train và Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chú ý: Trước khi thực hiện bước này phải check lại nhãn đã được gán bán thủ công ở bước trên trước**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_corpus = open('dataset/gold.txt', encoding='utf-8').readlines()\n",
    "new_line_idx = [i for i, item in enumerate(manual_tokens) if item == '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/train_gold.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in gold_corpus[:new_line_idx[49]]: f.write(line)\n",
    "    f.write('\\n')\n",
    "    \n",
    "with open('dataset/test_gold.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in gold_corpus[new_line_idx[49] + 1:]: f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/train_words.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in gold_corpus[:new_line_idx[49]]: f.write(re.sub('\\t.*', '', line))\n",
    "    f.write('\\n')        \n",
    "    \n",
    "with open('dataset/test_words.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in gold_corpus[new_line_idx[49] + 1:]: f.write(re.sub('\\t.*', '', line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Đọc dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(vocabs_dict, path):\n",
    "    data = []\n",
    "    file = open(path, encoding='utf-8').readlines()\n",
    "    \n",
    "    for index, word in enumerate(file):\n",
    "        if not word.split():\n",
    "            word = '--n--'\n",
    "            data.append(word)\n",
    "            continue\n",
    "        elif word.strip() not in vocabs_dict:\n",
    "            word = '--unk--'\n",
    "            data.append(word)\n",
    "            continue\n",
    "        data.append(word.strip())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tag_counts(gold):\n",
    "    tags = [word_tag.split()[1] for word_tag in gold if word_tag.split()]\n",
    "    tag_counts = pd.DataFrame(tags)[0].value_counts()\n",
    "    tag_counts.plot.bar(rot=0, width=0.7, legend=False, figsize=(15, 5))\n",
    "    return pd.DataFrame(tag_counts).T.assign(Total=tag_counts.sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Từ vựng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = open('resources/vocabs.txt', encoding='utf-8').read().split('\\n')\n",
    "vocabs_dict = {}\n",
    "index = 0\n",
    "\n",
    "for word in sorted(vocabs): \n",
    "    if word not in vocabs_dict: \n",
    "        vocabs_dict[word] = index  \n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng từ vựng: 54818\n",
      ": 0\n",
      "!: 1\n",
      "\": 2\n",
      "#: 3\n",
      "$: 4\n",
      "%: 5\n",
      "&: 6\n",
      "': 7\n",
      "'': 8\n",
      "'40s: 9\n",
      "'60s: 10\n",
      "'70s: 11\n",
      "'80s: 12\n",
      "'86: 13\n",
      "'90s: 14\n",
      "'N: 15\n",
      "'S: 16\n",
      "'d: 17\n",
      "'em: 18\n",
      "'ll: 19\n",
      "'m: 20\n"
     ]
    }
   ],
   "source": [
    "print('Số lượng từ vựng:', len(vocabs_dict.keys()))\n",
    "count = 0\n",
    "\n",
    "for key, value in vocabs_dict.items():\n",
    "    print(f'{key}: {value}')\n",
    "    count += 1\n",
    "    if count > 20: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tập Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng từ trong tập train_gold: 852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Pha\\tN\\n', 'lập_công\\tV\\n', 'trên\\tE\\n', 'đã\\tR\\n', 'giúp\\tV\\n']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gold = open('dataset/train_gold.txt', encoding='utf-8').readlines()\n",
    "print('Số lượng từ trong tập train_gold:', len(train_gold))\n",
    "train_gold[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng từ trong tập train_words: 852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Pha', 'lập_công', 'trên', 'đã', 'giúp']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words = preprocess(vocabs_dict, 'dataset/train_words.txt')\n",
    "print('Số lượng từ trong tập train_words:', len(train_words))\n",
    "train_words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các từ không nằm trong vocabs: "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>V</th>\n",
       "      <th>CH</th>\n",
       "      <th>E</th>\n",
       "      <th>A</th>\n",
       "      <th>R</th>\n",
       "      <th>Np</th>\n",
       "      <th>M</th>\n",
       "      <th>L</th>\n",
       "      <th>P</th>\n",
       "      <th>C</th>\n",
       "      <th>Cc</th>\n",
       "      <th>X</th>\n",
       "      <th>Nu</th>\n",
       "      <th>B</th>\n",
       "      <th>T</th>\n",
       "      <th>Nc</th>\n",
       "      <th>Z</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245</td>\n",
       "      <td>197</td>\n",
       "      <td>82</td>\n",
       "      <td>54</td>\n",
       "      <td>51</td>\n",
       "      <td>45</td>\n",
       "      <td>34</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     N    V  CH   E   A   R  Np   M   L   P   C  Cc  X  Nu  B  T  Nc  Z  Total\n",
       "0  245  197  82  54  51  45  34  21  19  15  14  13  6   2  1  1   1  1    802"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Các từ không nằm trong vocabs', end=': ')\n",
    "for word_tag, word in zip(train_gold, train_words):\n",
    "    if word == '--unk--': print(word_tag.split()[0], end=', ')\n",
    "plot_tag_counts(train_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tập Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng từ trong tập test_gold: 117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Những\\tL\\n', 'ngày\\tN\\n', 'đẹp_đẽ\\tA\\n', 'ấy\\tP\\n', ',\\tCH\\n']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gold = open('dataset/test_gold.txt', encoding='utf-8').readlines()\n",
    "print('Số lượng từ trong tập test_gold:', len(test_gold))\n",
    "test_gold[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng từ trong tập test_words: 117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Những', 'ngày', 'đẹp_đẽ', 'ấy', ',']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words = preprocess(vocabs_dict, 'dataset/test_words.txt')\n",
    "print('Số lượng từ trong tập test_words:', len(test_words))\n",
    "test_words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các từ không nằm trong vocabs: Giáng_sinh, năm_qua, gần_đây, Tân_Sơn_Nhất, "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>V</th>\n",
       "      <th>CH</th>\n",
       "      <th>P</th>\n",
       "      <th>A</th>\n",
       "      <th>R</th>\n",
       "      <th>E</th>\n",
       "      <th>L</th>\n",
       "      <th>X</th>\n",
       "      <th>Np</th>\n",
       "      <th>Cc</th>\n",
       "      <th>M</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    N   V  CH   P   A  R  E  L  X  Np  Cc  M  Total\n",
       "0  31  15  12  12  11  8  7  3  3   2   2  1    107"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Các từ không nằm trong vocabs', end=': ')\n",
    "for word_tag, word in zip(test_gold, test_words):\n",
    "    if word == '--unk--': print(word_tag.split()[0], end=', ')\n",
    "plot_tag_counts(test_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_word_tag(word_tag, vocabs_dict): \n",
    "    if not word_tag.split():\n",
    "        word = '--n--'\n",
    "        tag = '--s--'\n",
    "    else:\n",
    "        word, tag = word_tag.split()\n",
    "        if word not in vocabs_dict: word = '--unk--'\n",
    "    return word, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(train_gold, vocab):\n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    \n",
    "    prev_tag = '--s--' \n",
    "    for word_tag in train_gold:\n",
    "        word, tag = seperate_word_tag(word_tag, vocab) \n",
    "        \n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        emission_counts[(tag, word)] += 1\n",
    "        tag_counts[tag] += 1\n",
    "        prev_tag = tag\n",
    "    return transition_counts, emission_counts, tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số nhãn: 19\n",
      "['--s--', 'A', 'B', 'C', 'CH', 'Cc', 'E', 'L', 'M', 'N', 'Nc', 'Np', 'Nu', 'P', 'R', 'T', 'V', 'X', 'Z']\n"
     ]
    }
   ],
   "source": [
    "transition_counts, emission_counts, tag_counts = create_dictionaries(train_gold, vocabs_dict)\n",
    "states = sorted(tag_counts.keys())\n",
    "print('Số nhãn:', len(states))\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition examples: \n",
      "(('--s--', 'N'), 28)\n",
      "(('N', 'V'), 68)\n",
      "(('V', 'E'), 21)\n"
     ]
    }
   ],
   "source": [
    "print(\"Transition examples: \")\n",
    "for example in list(transition_counts.items())[:3]:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emission examples: \n",
      "(('N', 'Pha'), 1)\n",
      "(('V', 'lập_công'), 1)\n",
      "(('E', 'trên'), 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Emission examples: \")\n",
    "for example in list(emission_counts.items())[:3]:\n",
    "    print (example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pos(words, gold, emission_counts, vocabs_dict, states):\n",
    "    num_correct = 0\n",
    "    all_words = set(emission_counts.keys())\n",
    "    \n",
    "    for word, gold_tuple in zip(words, gold): \n",
    "        gold_tuple_list = gold_tuple.split()\n",
    "        if len(gold_tuple_list) != 2: continue\n",
    "        else: true_label = gold_tuple_list[1]\n",
    "    \n",
    "        count_final = 0\n",
    "        pos_final = ''\n",
    "        if word not in vocabs_dict: continue\n",
    "        \n",
    "        for pos in states:\n",
    "            if (pos, word) not in emission_counts: continue\n",
    "            count = emission_counts[(pos, word)]\n",
    "            \n",
    "            if count > count_final:\n",
    "                count_final = count\n",
    "                pos_final = pos\n",
    "                    \n",
    "        if pos_final == true_label: num_correct += 1\n",
    "    accuracy = num_correct / len(gold)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Độ chính xác trên tập train: 0.9330985915492958\n"
     ]
    }
   ],
   "source": [
    "accuracy = predict_pos(train_words, train_gold, emission_counts, vocabs_dict, states)\n",
    "print('Độ chính xác trên tập train:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Độ chính xác trên tập test: 0.4188034188034188\n"
     ]
    }
   ],
   "source": [
    "accuracy = predict_pos(test_words, test_gold, emission_counts, vocabs_dict, states)\n",
    "print('Độ chính xác trên tập test:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Ma trận xác suất Hidden Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ma trận chuyển tiếp 'A' (transition matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    num_tags = len(all_tags)\n",
    "    \n",
    "    A = np.zeros((num_tags, num_tags))\n",
    "    trans_keys = set(transition_counts.keys())\n",
    "    \n",
    "    for i in range(num_tags):\n",
    "        for j in range(num_tags):\n",
    "            count = 0\n",
    "            key = (all_tags[i], all_tags[j])\n",
    "            if key in transition_counts: count = transition_counts[key]\n",
    "                \n",
    "            count_prev_tag = tag_counts[all_tags[i]]\n",
    "            A[i, j] = (count + alpha) / (count_prev_tag + alpha * num_tags)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cc</th>\n",
       "      <th>E</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cc</th>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.076888</td>\n",
       "      <td>0.076888</td>\n",
       "      <td>0.230509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>0.018531</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.092579</td>\n",
       "      <td>0.055555</td>\n",
       "      <td>0.536867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.999054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.047624</td>\n",
       "      <td>0.047624</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.142776</td>\n",
       "      <td>0.523384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>0.016329</td>\n",
       "      <td>0.077549</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.020411</td>\n",
       "      <td>0.232639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Cc         E         L         M         N\n",
       "Cc  0.000077  0.000077  0.076888  0.076888  0.230509\n",
       "E   0.018531  0.000019  0.092579  0.055555  0.536867\n",
       "L   0.000053  0.000053  0.000053  0.000053  0.999054\n",
       "M   0.047624  0.047624  0.000048  0.142776  0.523384\n",
       "N   0.016329  0.077549  0.000004  0.020411  0.232639"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "for i in range(len(states)): tag_counts.pop(i, None)\n",
    "    \n",
    "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "df = pd.DataFrame(\n",
    "    A[5:10, 5:10], \n",
    "    index = states[5:10], \n",
    "    columns = states[5:10]\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ma trận phát xạ 'B' (emission matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocabs):\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    num_tags = len(tag_counts)\n",
    "    num_words = len(vocabs)\n",
    "    \n",
    "    B = np.zeros((num_tags, num_words))\n",
    "    emis_keys = set(list(emission_counts.keys()))\n",
    "    \n",
    "    for i in range(num_tags):\n",
    "        for j in range(num_words):\n",
    "            count = 0\n",
    "            key = (all_tags[i], vocabs[j])\n",
    "            if key in emission_counts.keys(): count = emission_counts[key]\n",
    "                \n",
    "            count_tag = tag_counts[all_tags[i]]\n",
    "            B[i, j] = (count + alpha) / (count_tag + alpha * num_words)\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thông_báo</th>\n",
       "      <th>hạt_nhân</th>\n",
       "      <th>tinh_thần</th>\n",
       "      <th>lập_công</th>\n",
       "      <th>vị_trí</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>0.003339</td>\n",
       "      <td>0.006674</td>\n",
       "      <td>0.003339</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.010009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.003975</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CH</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cc</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    thông_báo  hạt_nhân  tinh_thần  lập_công    vị_trí\n",
       "N    0.003339  0.006674   0.003339  0.000003  0.010009\n",
       "V    0.000004  0.000004   0.000004  0.003975  0.000004\n",
       "CH   0.000007  0.000007   0.000007  0.000007  0.000007\n",
       "Cc   0.000015  0.000015   0.000015  0.000015  0.000015\n",
       "E    0.000009  0.000009   0.000009  0.000009  0.000009"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cidx  = ['thông_báo', 'hạt_nhân', 'tinh_thần', 'lập_công', 'vị_trí']\n",
    "rvals = ['N', 'V', 'CH', 'Cc', 'E', 'L']\n",
    "cols = [vocabs_dict[word] for word in cidx]\n",
    "rows = [states.index(tag) for tag in rvals]\n",
    "\n",
    "for i in range(len(states)): tag_counts.pop(i, None)\n",
    "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocabs_dict))\n",
    "\n",
    "df = pd.DataFrame(B[np.ix_(rows, cols)], index=rvals, columns=cidx)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Thuật toán Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do 2 ma trận A, B đều sẽ tính toán xác suất cho nhãn --s-- (nhãn bắt đầu) bình thường như các nhãn khác nên lúc dự đoán nhãn --s-- cũng có thể sẽ được gán cho bất kỳ từ nào. Nếu muốn nhãn --s-- không được gán khi dự đoán thì:\n",
    "- Bỏ qua cột đầu khi sử dụng ma trận A => Khi sử dụng lại ma trận A trong thuật toán Viterbi phải trừ đi 1 ở cột\n",
    "- Bỏ qua hàng đầu khi sử dụng ma trận B => Khi sử dụng lại ma trận B trong thuật toán Viterbi phải trừ đi 1 ở hàng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([sublist[1:].tolist() for sublist in A])\n",
    "B = B[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bước Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_initialize(states, tag_counts, A, B, corpus, vocabs_dict):\n",
    "    num_tags = len(tag_counts)\n",
    "    s_idx = states.index('--s--')\n",
    "    \n",
    "    best_probs = np.zeros((num_tags, len(corpus)))\n",
    "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
    "    \n",
    "    for i in range(num_tags):\n",
    "        if A[s_idx, i - 1] == 0: best_probs[i, 0] = float('-inf')\n",
    "        else: \n",
    "            index = vocabs_dict[corpus[0]]\n",
    "            # best_probs[i, 0] = math.log(A[s_idx, i]) + math.log(B[i, index])\n",
    "            best_probs[i, 0] = math.log(A[s_idx, i - 1]) + math.log(B[i - 1, index])\n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs_train[0, 0]: -21.750009889225183\n",
      "best_paths_train[2, 3]: 0\n"
     ]
    }
   ],
   "source": [
    "best_probs_train, best_paths_train = viterbi_initialize(states, tag_counts, A, B, train_words, vocabs_dict)\n",
    "print('best_probs_train[0, 0]:', best_probs_train[0, 0]) \n",
    "print('best_paths_train[2, 3]:', best_paths_train[2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs_test[0, 0]: -21.750009889225183\n",
      "best_paths_test[2, 3]: 0\n"
     ]
    }
   ],
   "source": [
    "best_probs_test, best_paths_test = viterbi_initialize(states, tag_counts, A, B, test_words, vocabs_dict)\n",
    "print('best_probs_test[0, 0]:', best_probs_test[0, 0]) \n",
    "print('best_paths_test[2, 3]:', best_paths_test[2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bước Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_forward(A, B, corpus, best_probs, best_paths, vocabs_dict):\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    for i in range(1, len(corpus)): \n",
    "        if i % 5000 == 0: print(f'Processed {i} words...')\n",
    "            \n",
    "        for j in range(num_tags):\n",
    "            best_prob_i = float('-inf')\n",
    "            best_path_i = None\n",
    "            \n",
    "            for k in range(num_tags):\n",
    "                index = vocabs_dict[corpus[i]]\n",
    "                # prob = best_probs[k, i - 1] + math.log(A[k, j]) + math.log(B[j, index])\n",
    "                prob = best_probs[k, i - 1] + math.log(A[k, j - 1]) + math.log(B[j - 1, index])\n",
    "\n",
    "                if prob > best_prob_i:\n",
    "                    best_prob_i = prob\n",
    "                    best_path_i = k\n",
    "                    \n",
    "            best_probs[j, i] = best_prob_i\n",
    "            best_paths[j, i] = best_path_i\n",
    "            \n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs_train[0, 1]: -29.62128154751998\n",
      "best_paths_train[0, 4]: 14\n"
     ]
    }
   ],
   "source": [
    "best_probs_train, best_paths_train = viterbi_forward(A, B, train_words, best_probs_train, best_paths_train, vocabs_dict)\n",
    "print('best_probs_train[0, 1]:', best_probs_train[0, 1]) \n",
    "print('best_paths_train[0, 4]:', best_paths_train[0, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs_test[0, 1]: -27.386413580260943\n",
      "best_paths_test[0, 4]: 16\n"
     ]
    }
   ],
   "source": [
    "best_probs_test, best_paths_test = viterbi_forward(A, B, test_words, best_probs_test, best_paths_test, vocabs_dict)\n",
    "print('best_probs_test[0, 1]:', best_probs_test[0, 1]) \n",
    "print('best_paths_test[0, 4]:', best_paths_test[0, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bước Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
    "    m = best_paths.shape[1] \n",
    "    z = [None] * m\n",
    "    pred = [None] * m\n",
    "    \n",
    "    best_prob_for_last_word = float('-inf')\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    for k in range(num_tags):\n",
    "        if best_probs[k, m - 1] > best_prob_for_last_word:\n",
    "            best_prob_for_last_word = best_probs[k, m - 1]\n",
    "            z[m - 1] = k\n",
    "            \n",
    "    pred[m - 1] = states[z[m - 1]]\n",
    "    for i in range(m - 1, -1, -1):\n",
    "        z[i - 1] = best_paths[z[i], i]\n",
    "        pred[i - 1] = states[z[i - 1]]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dự đoán cho test_pred[-7:116]:\n",
      "['công_cụ', 'không_thể', 'đảo_ngược', 'trong', 'cuộc_sống', '.']\n",
      "['Np', 'R', 'V', 'E', 'N', 'CH']\n",
      "Dự đoán cho test_pred[0:7]:\n",
      "['Những', 'ngày', 'đẹp_đẽ', 'ấy', ',', 'bố', 'luôn']\n",
      "['L', 'N', 'Cc', 'A', 'CH', 'P', 'R']\n"
     ]
    }
   ],
   "source": [
    "train_pred = viterbi_backward(best_probs_train, best_paths_train, train_words, states)\n",
    "test_pred = viterbi_backward(best_probs_test, best_paths_test, test_words, states)\n",
    "m = len(test_pred)\n",
    "\n",
    "print(f'Dự đoán cho test_pred[-7:{m - 1}]:')\n",
    "print(test_words[-7:m-1])\n",
    "print(test_pred[-7:m-1])\n",
    "\n",
    "print('Dự đoán cho test_pred[0:7]:')\n",
    "print(test_words[0:7])\n",
    "print(test_pred[0:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Đánh giá kết quả"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kết quả của mô hình Hidden Markov kết hợp thuật toán Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Những/L ngày/N đẹp_đẽ/Cc ấy/A ,/CH bố/P luôn/R dành/V thời_gian/N để/E vui_vẻ/N với/E anh_em/N tôi/P ./CH \n",
      "Trong/E khi/N chờ/E nước/N nóng/A ,/CH tôi/P pha/V rượu/N táo/Cc nóng/A ./CH \n",
      "Hàng/M trăm/M người/N đến/V dùng/E tiệc/M tự_chọn/Nu ở/E chỗ/N mẹ/E những/L ngày/N --unk--/A ./CH \n",
      "Năm/R nào/V cũng/R vậy/A hoặc/Cc có_vẻ/A như_vậy/A ./CH \n",
      "Tôi/P chưa/R bao_giờ/V thực_sự/E cảm_thấy/M mệt_mỏi/Nu vì/CH nó/P ./CH \n",
      "Tôi/P đã/R có_thể/R chống/V lại/E cảm_giác/N đó/V trong/E nhiều/A --unk--/A ./CH \n",
      "Những/L năm/N --unk--/CH tôi/P qua_lại/R sân_bay/A --unk--/Cc thường_xuyên/A ./CH \n",
      "Trường/N tôi/P từng/R nghiêm_cấm/V sử_dụng/V điện_thoại/V trong/E lớp/N ./CH \n",
      "Học_sinh/R ngày_nay/V có_thể/R dễ_dàng/A tiếp_cận/Cc bài_học/A và/Cc phương_pháp/A giải/Cc bài_tập/A ./CH \n",
      "Điện_thoại/R thông_minh/V là/V công_cụ/Np không_thể/R đảo_ngược/V trong/E cuộc_sống/N ./CH \n"
     ]
    }
   ],
   "source": [
    "for word, tag in zip(test_words, test_pred):\n",
    "    if word == '--n--': print()\n",
    "    else: print(f'{word}/{tag}', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def report(pred, gold):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for prediction, word_tag in zip(pred, gold):\n",
    "        word_tag_tuple = word_tag.split()\n",
    "        if len(word_tag_tuple) != 2: continue \n",
    "\n",
    "        word, tag = word_tag_tuple\n",
    "        y_pred.append(prediction)\n",
    "        y_true.append(tag)\n",
    "        \n",
    "    print(classification_report(y_pred, y_true))\n",
    "    return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết quả của mô hình Hidden Markov kết hợp thuật toán Viterbi trên tập train:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      1.00      1.00        51\n",
      "           B       1.00      1.00      1.00         1\n",
      "           C       0.93      0.93      0.93        14\n",
      "          CH       1.00      1.00      1.00        82\n",
      "          Cc       1.00      1.00      1.00        13\n",
      "           E       0.91      0.96      0.93        51\n",
      "           L       1.00      1.00      1.00        19\n",
      "           M       1.00      1.00      1.00        21\n",
      "           N       1.00      0.99      1.00       247\n",
      "          Nc       1.00      1.00      1.00         1\n",
      "          Np       1.00      1.00      1.00        34\n",
      "          Nu       1.00      1.00      1.00         2\n",
      "           P       1.00      1.00      1.00        15\n",
      "           R       1.00      0.98      0.99        46\n",
      "           T       1.00      1.00      1.00         1\n",
      "           V       0.99      0.99      0.99       198\n",
      "           X       0.83      1.00      0.91         5\n",
      "           Z       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.99       802\n",
      "   macro avg       0.98      0.99      0.99       802\n",
      "weighted avg       0.99      0.99      0.99       802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Kết quả của mô hình Hidden Markov kết hợp thuật toán Viterbi trên tập train:\\n')\n",
    "y_pred, y_true_train = report(train_pred, train_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết quả của mô hình Hidden Markov kết hợp thuật toán Viterbi trên tập test:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.45      0.33      0.38        15\n",
      "          CH       1.00      0.86      0.92        14\n",
      "          Cc       1.00      0.29      0.44         7\n",
      "           E       0.86      0.50      0.63        12\n",
      "           L       1.00      1.00      1.00         3\n",
      "           M       1.00      0.25      0.40         4\n",
      "           N       0.45      0.93      0.61        15\n",
      "          Np       0.00      0.00      0.00         1\n",
      "          Nu       0.00      0.00      0.00         2\n",
      "           P       0.58      0.88      0.70         8\n",
      "           R       0.88      0.58      0.70        12\n",
      "           V       0.53      0.57      0.55        14\n",
      "           X       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.61       107\n",
      "   macro avg       0.60      0.48      0.49       107\n",
      "weighted avg       0.70      0.61      0.61       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Kết quả của mô hình Hidden Markov kết hợp thuật toán Viterbi trên tập test:\\n')\n",
    "y_pred, y_true_test = report(test_pred, test_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kết quả khi sử dụng thư viện VnCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Những/L ngày/N đẹp_đẽ/A ấy/P ,/CH bố/N luôn/R dành/V thời_gian/N để/E vui_vẻ/A với/E anh_em/N tôi/P ./CH \n",
      "Trong/E khi/N chờ/V nước/N nóng/A ,/CH tôi/P pha/V rượu/N táo/V nóng/A ./CH \n",
      "Hàng/N trăm/M người/N đến/V dùng/V tiệc/N tự_chọn/P ở/E chỗ/N mẹ/N những/L ngày/N Giáng_sinh/Np ./CH \n",
      "Năm/Np nào/P cũng/R vậy/P hoặc/Cc có_vẻ/X như_vậy/X ./CH \n",
      "Tôi/P chưa/R bao_giờ/P thực_sự/A cảm_thấy/V mệt_mỏi/A vì/E nó/P ./CH \n",
      "Tôi/P đã/R có_thể/R chống/V lại/R cảm_giác/N đó/P trong/E nhiều/A năm_qua/N ./CH \n",
      "Những/L năm/N gần_đây/A tôi/P qua_lại/V sân_bay/N Tân_Sơn_Nhất/Np thường_xuyên/A ./CH \n",
      "Trường/Np tôi/P từng/P nghiêm_cấm/V sử_dụng/V điện_thoại/N trong/E lớp/N ./CH \n",
      "Học_sinh/N ngày_nay/N có_thể/R dễ_dàng/A tiếp_cận/V bài_học/N và/Cc phương_pháp/N giải/N bài_tập/N ./CH \n",
      "Điện_thoại/N thông_minh/A là/V công_cụ/N không_thể/R đảo_ngược/V trong/E cuộc_sống/N ./CH \n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "for word_tag in test_gold:\n",
    "    word_tag_tuple = word_tag.split()\n",
    "    if len(word_tag_tuple) != 2: \n",
    "        print()\n",
    "        continue\n",
    "\n",
    "    word, tag = word_tag_tuple\n",
    "    if '_' not in word: pred = client.pos_tag(word)\n",
    "    else: pred = client.pos_tag(word.replace('_', ' '))\n",
    "\n",
    "    print(f'{word}/{pred[0][0][1]}', end=' ') \n",
    "    y_pred.append(pred[0][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết quả khi sử dụng thư viện VnCoreNLP trên tập test:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.91      0.91      0.91        11\n",
      "          CH       1.00      1.00      1.00        12\n",
      "          Cc       1.00      1.00      1.00         2\n",
      "           E       1.00      0.88      0.93         8\n",
      "           L       1.00      1.00      1.00         3\n",
      "           M       1.00      1.00      1.00         1\n",
      "           N       0.87      0.96      0.92        28\n",
      "          Np       1.00      0.50      0.67         4\n",
      "           P       1.00      0.86      0.92        14\n",
      "           R       0.88      0.88      0.88         8\n",
      "           V       0.87      0.93      0.90        14\n",
      "           X       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.92       107\n",
      "   macro avg       0.93      0.91      0.91       107\n",
      "weighted avg       0.92      0.92      0.91       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Kết quả khi sử dụng thư viện VnCoreNLP trên tập test:\\n')\n",
    "print(classification_report(y_pred, y_true_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
